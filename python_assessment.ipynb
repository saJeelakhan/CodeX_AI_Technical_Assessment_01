{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfnSVWmx84crqmanin+N2w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saJeelakhan/CodeX_AI_Technical_Assessment_01/blob/main/python_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApQp5v4BiNzs",
        "outputId": "00984899-a1cd-4064-a875-d63293c99383"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.1.31)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import praw\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import random\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "DP25XDjXluei"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPSedjoxl4Dn",
        "outputId": "505c3529-8b12-4ed2-f79f-985a1fc5fdcd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility Functions\n",
        "def is_valid_greeklish(text):\n",
        "    if len(text) < 3 or text in [\"[deleted]\", \"[removed]\"]:\n",
        "        return False\n",
        "    if re.search(r'[α-ωά-ώΑ-Ω]', text) or re.search(r'[а-яА-Я]', text):\n",
        "        return False\n",
        "    greeklish_words = {\"kaneis\", \"einai\", \"thelw\", \"gia\", \"kai\", \"den\", \"ti\", \"sou\", \"mou\", \"ellinika\", \"greeklish\", \"etsi\", \"prepei\", \"pame\", \"mporei\", \"xerei\", \"thes\", \"opote\"}\n",
        "    words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
        "    return len(words & greeklish_words) >= 2\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def split_into_sentences(paragraph):\n",
        "    return [s.strip() for s in paragraph.replace('!', '.').replace('?', '.').split('.') if s.strip()]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "y8SLIrTJmMqm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greeklish Sources\n",
        "def scrape_reddit_greeklish():\n",
        "    print(\"Scraping Reddit Greeklish posts...\")\n",
        "    reddit = praw.Reddit(client_id='d8QlBknjt1u9nDgmmo1PXg', client_secret='HdaHhnFhFCkPk0O5FCN32CS2K-YWhA', user_agent='GreeklishScraper', check_for_async=False)\n",
        "    posts = []\n",
        "    subreddit = reddit.subreddit(\"greece\")\n",
        "    for submission in subreddit.search(\"greeklish\", limit=300):\n",
        "        if is_valid_greeklish(submission.title):\n",
        "            posts.append(submission.title)\n",
        "        if submission.comments:\n",
        "            submission.comments.replace_more(limit=0)\n",
        "            for comment in submission.comments.list():\n",
        "                if is_valid_greeklish(comment.body):\n",
        "                    posts.append(comment.body)\n",
        "        if len(posts) >= 200:\n",
        "            break\n",
        "    print(f\"Fetched {len(posts)} Greeklish posts from r/greece\")\n",
        "    return posts[:200]\n",
        "\n",
        "def scrape_insomnia():\n",
        "    print(\"Scraping Greeklish from Insomnia.gr...\")\n",
        "    url = 'https://www.insomnia.gr/forums/'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    posts = soup.find_all('p')\n",
        "    greeklish = [post.text for post in posts if is_valid_greeklish(post.text)]\n",
        "    print(f\"Fetched {len(greeklish)} Greeklish posts from Insomnia.gr\")\n",
        "    return greeklish[:100]\n",
        "\n",
        "def scrape_youtube():\n",
        "    print(\"Scraping Greeklish YouTube comments...\")\n",
        "    url = 'https://www.youtube.com/watch?v=_akH1Bns2B8'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    comments = soup.find_all('yt-formatted-string', class_='style-scope ytd-comment-renderer')\n",
        "    greeklish = [comment.text for comment in comments if is_valid_greeklish(comment.text)]\n",
        "    print(f\"Fetched {len(greeklish)} Greeklish YouTube comments\")\n",
        "    return greeklish[:100]"
      ],
      "metadata": {
        "id": "BdCz10oampi1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# English Sources\n",
        "def scrape_reddit_english():\n",
        "    print(\"Scraping Reddit English posts...\")\n",
        "    reddit = praw.Reddit(client_id='d8QlBknjt1u9nDgmmo1PXg', client_secret='HdaHhnFhFCkPk0O5FCN32CS2K-YWhA', user_agent='EnglishScraper', check_for_async=False)\n",
        "    english = []\n",
        "    subreddit = reddit.subreddit(\"AskReddit\")\n",
        "    for submission in subreddit.top(limit=100):\n",
        "        english.extend(nltk.sent_tokenize(submission.title))\n",
        "        submission.comments.replace_more(limit=0)\n",
        "        for comment in submission.comments.list():\n",
        "            if hasattr(comment, 'body'):\n",
        "                english.extend(nltk.sent_tokenize(comment.body))\n",
        "        if len(english) >= 300:\n",
        "            break\n",
        "    print(f\"Fetched {len(english)} English sentences from r/AskReddit\")\n",
        "    return english[:300]\n",
        "\n",
        "def scrape_wikipedia_sentences(url, min_sentences=150):\n",
        "    print(\"Scraping English from Wikipedia...\")\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    paragraphs = soup.find_all('p')\n",
        "    sentences = []\n",
        "    for para in paragraphs:\n",
        "        text = para.get_text()\n",
        "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "        sentences.extend(nltk.sent_tokenize(text))\n",
        "        if len(sentences) >= min_sentences:\n",
        "            break\n",
        "    print(f\"Fetched {len(sentences)} English Wikipedia sentences\")\n",
        "    return sentences[:min_sentences]"
      ],
      "metadata": {
        "id": "DnQpq1BOmsko"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Collection & Preprocessing\n",
        "def collect_data():\n",
        "    greeklish = scrape_reddit_greeklish() + scrape_insomnia() + scrape_youtube()\n",
        "    english = scrape_reddit_english() + scrape_wikipedia_sentences(\"https://en.wikipedia.org/wiki/Natural_language_processing\", 200)\n",
        "    raw_df = pd.DataFrame({\n",
        "        'text': greeklish + english,\n",
        "        'label': ['Greeklish'] * len(greeklish) + ['English'] * len(english)\n",
        "    })\n",
        "    print(\"Splitting and cleaning sentences...\")\n",
        "    new_rows = []\n",
        "    for _, row in raw_df.iterrows():\n",
        "        for sentence in split_into_sentences(row['text']):\n",
        "            cleaned = preprocess_text(sentence)\n",
        "            if cleaned:\n",
        "                new_rows.append({'sentence': cleaned, 'label': row['label']})\n",
        "\n",
        "    df = pd.DataFrame(new_rows)\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    df.to_csv(\"dataset.csv\", index=False)\n",
        "    print(f\"Final dataset: {len(df)} rows\")\n",
        "    print(df['label'].value_counts())\n",
        "    return df"
      ],
      "metadata": {
        "id": "PPKsg1wpm7bl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "def train_model(df):\n",
        "    X = df['sentence']\n",
        "    y = df['label']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    print(\"\\nModel Performance:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "    print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "\n",
        "    os.makedirs('model', exist_ok=True)\n",
        "    joblib.dump(model, 'model/greeklish_classifier.pkl')\n",
        "    joblib.dump(vectorizer, 'model/tfidf_vectorizer.pkl')\n",
        "    print(\"\\nModel and vectorizer saved to the 'model' directory.\")\n",
        "    return model, vectorizer"
      ],
      "metadata": {
        "id": "JjFbIgdDm_V4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, vectorizer):\n",
        "    os.makedirs('model', exist_ok=True)\n",
        "    joblib.dump(model, 'model/greeklish_classifier.pkl')\n",
        "    joblib.dump(vectorizer, 'model/tfidf_vectorizer.pkl')\n",
        "    print(\"\\nModel and vectorizer saved successfully in 'model/' directory.\")"
      ],
      "metadata": {
        "id": "lz1usvXG4FpC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Function\n",
        "def predict_text(text):\n",
        "    model = joblib.load(\"model/greeklish_classifier.pkl\")\n",
        "    vectorizer = joblib.load(\"model/tfidf_vectorizer.pkl\")\n",
        "    processed_text = preprocess_text(text)\n",
        "    prediction = model.predict(vectorizer.transform([processed_text]))[0]\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "-jLzyBJDnETR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iVxybrQiEr9",
        "outputId": "2fc91bd7-707d-42a0-c8b7-65ef7639ba22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data collection and training pipeline...\n",
            "\n",
            "Scraping Reddit Greeklish posts...\n",
            "Fetched 181 Greeklish posts from r/greece\n",
            "Scraping Greeklish from Insomnia.gr...\n",
            "Fetched 0 Greeklish posts from Insomnia.gr\n",
            "Scraping Greeklish YouTube comments...\n",
            "Fetched 0 Greeklish YouTube comments\n",
            "Scraping Reddit English posts...\n",
            "Fetched 918 English sentences from r/AskReddit\n",
            "Scraping English from Wikipedia...\n",
            "Fetched 38 English Wikipedia sentences\n",
            "Splitting and cleaning sentences...\n",
            "Final dataset: 844 rows\n",
            "label\n",
            "Greeklish    481\n",
            "English      363\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training model...\n",
            "\n",
            "Model Performance:\n",
            "Accuracy: 0.9467\n",
            "Precision: 0.9526\n",
            "Recall: 0.9467\n",
            "F1-Score: 0.9470\n",
            "\n",
            "Model and vectorizer saved to the 'model' directory.\n",
            "\n",
            "Testing prediction examples:\n",
            "ti kaneis -> Greeklish\n",
            "Hello, how are you? -> English\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# Main Script\n",
        "def main():\n",
        "    print(\"Starting data collection and training pipeline...\\n\")\n",
        "    df = collect_data()\n",
        "    print(\"\\nTraining model...\")\n",
        "    model, vectorizer = train_model(df)\n",
        "    print(\"\\nTesting prediction examples:\")\n",
        "    print(\"ti kaneis ->\", predict_text(\"ti kaneis\"))  # Greeklish\n",
        "    print(\"Hello, how are you? ->\", predict_text(\"Hello, how are you?\"))  # English\n",
        "    print(\"\\nDone.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}